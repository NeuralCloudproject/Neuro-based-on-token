import cupy as cp
import string
import requests
import numpy as np

class NeuronLayer:
    """ë‰´ëŸ° ë ˆì´ì–´ (ì´ì˜¨ ë†ë„, ì´ì˜¨ íŒí”„, ì´ì˜¨ ì±„ë„ í¬í•¨)"""
    def __init__(self, num_neurons):
        self.num_neurons = num_neurons
        self.potentials = cp.full((num_neurons,), -70.0)  # ë§‰ì „ìœ„
        self.thresholds = cp.full((num_neurons,), -55.0)  # ë°œí™” ì„ê³„ê°’
        self.absolute_refractory_period = cp.zeros(num_neurons)  # ë¶ˆì‘ê¸°

        # ğŸ”¹ ì´ì˜¨ ë†ë„ (ë‚´ë¶€ & ì™¸ë¶€)
        self.na_in = cp.full((num_neurons,), 15.0)
        self.na_out = cp.full((num_neurons,), 145.0)
        self.k_in = cp.full((num_neurons,), 140.0)
        self.k_out = cp.full((num_neurons,), 4.0)
        self.ca_in = cp.full((num_neurons,), 0.0001)
        self.ca_out = cp.full((num_neurons,), 2.0)
        self.cl_in = cp.full((num_neurons,), 10.0)
        self.cl_out = cp.full((num_neurons,), 110.0)
        self.h_concentration = cp.full((num_neurons,), 7.2)  # pH

        # ğŸ”¹ ì´ì˜¨ ì±„ë„ (Naâº, Kâº, CaÂ²âº, Clâ», Hâº)
        self.na_channel_open = cp.zeros(num_neurons, dtype=bool)
        self.k_channel_open = cp.zeros(num_neurons, dtype=bool)
        self.ca_channel_open = cp.zeros(num_neurons, dtype=bool)
        self.cl_channel_open = cp.zeros(num_neurons, dtype=bool)
        self.h_channel_open = cp.zeros(num_neurons, dtype=bool)

    def update_potential(self, time_step=0.1):
        """ë§‰ì „ìœ„ ì—…ë°ì´íŠ¸"""
        self.potentials[self.na_channel_open] += 10.0 * time_step
        self.potentials[self.k_channel_open] -= 10.0 * time_step
        self.potentials[self.ca_channel_open] += 2.0 * time_step
        self.potentials[self.cl_channel_open] -= 5.0 * time_step
        self.potentials[self.h_channel_open] += 0.5 * time_step  # Hâº í†µë¡œ ê°œë°© ì‹œ ë§‰ì „ìœ„ ìƒìŠ¹

        self.potentials = cp.clip(self.potentials, -80.0, 50.0)

    def update_ion_pumps(self):
        """ì´ì˜¨ íŒí”„ (Naâº/Kâº, CaÂ²âº, Hâº, ë…ì„± ì œê±°)"""
        pump_activity = 0.1  
        self.na_in += pump_activity * (self.na_out - self.na_in) * 0.1
        self.k_in -= pump_activity * (self.k_out - self.k_in) * 0.1
        self.ca_in -= 0.05 * self.ca_in
        self.h_concentration -= 0.02 * (self.h_concentration - 7.2)

    def fire(self):
        """ë‰´ëŸ° ë°œí™”"""
        fired = self.potentials >= self.thresholds
        self.potentials[fired] = -70.0  # ë°œí™” í›„ ì´ˆê¸°í™”
        return fired

    def update(self, time_step=0.1):
        """ì „ì²´ ë‰´ëŸ° ìƒíƒœ ì—…ë°ì´íŠ¸"""
        self.update_potential(time_step)
        self.update_ion_pumps()


class TokenLayer:
    """ì…ë ¥ì¸µ (ë¬¸ì í† í°í™” ë° ë‰´ëŸ° ë³€í™˜)"""
    def __init__(self):
        self.alphabet = list(string.ascii_uppercase) + list(string.digits) + ['_', '.', ',', '!', '?']
        self.num_tokens = len(self.alphabet)
        self.neurons = NeuronLayer(self.num_tokens)

    def tokenize(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì í† í°ìœ¼ë¡œ ë³€í™˜"""
        return [char.upper() if char.upper() in self.alphabet else '_' for char in text]

    def encode(self, tokens):
        """í† í°ì„ ë‰´ëŸ° ì¸ë±ìŠ¤ë¡œ ë³€í™˜"""
        return [self.alphabet.index(token) for token in tokens]

    def decode(self, indices):
        """ë‰´ëŸ° ì¸ë±ìŠ¤ë¥¼ ë¬¸ìë¡œ ë³€í™˜"""
        return ''.join([self.alphabet[i] for i in indices])


class Synapse:
    """ë‰´ëŸ° ê°„ ì‹œëƒ…ìŠ¤ ì—°ê²°"""
    def __init__(self, pre_layer, post_layer):
        self.pre_layer = pre_layer
        self.post_layer = post_layer
        self.weights = cp.random.uniform(0.1, 1.0, size=(pre_layer.num_neurons, post_layer.num_neurons))

    def propagate(self):
        """ì‹ í˜¸ ì „ë‹¬"""
        signals = cp.dot(self.pre_layer.potentials, self.weights)
        self.post_layer.potentials += signals


class NLPModel:
    """ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ (ì…ë ¥ì¸µ -> ì—°ì‚°ì¸µ(ë‹¤ì¤‘ ë ˆì´ì–´) -> ì¶œë ¥ì¸µ)"""
    def __init__(self, num_layers=3):
        self.token_layer = TokenLayer()
        self.processing_layers = [NeuronLayer(self.token_layer.num_tokens) for _ in range(num_layers)]
        self.output_layer = NeuronLayer(self.token_layer.num_tokens)

        # ì‹œëƒ…ìŠ¤ ì—°ê²°
        self.synapses = [Synapse(self.token_layer.neurons, self.processing_layers[0])]
        for i in range(num_layers - 1):
            self.synapses.append(Synapse(self.processing_layers[i], self.processing_layers[i + 1]))
        self.synapses.append(Synapse(self.processing_layers[-1], self.output_layer))

    def train(self, texts, epochs=5000, batch_size=32):
        """ëŒ€ëŸ‰ ë°ì´í„° í•™ìŠµ (GPT-2ì™€ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ëª©í‘œë¡œ)"""
        training_sentences = [self.token_layer.tokenize(text) for text in texts]
        training_indices = [self.token_layer.encode(sentence) for sentence in training_sentences]

        # âœ… 1. ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´ ê³„ì‚°
        max_length = max(len(indices) for indices in training_indices)

        # âœ… 2. íŒ¨ë”© ì ìš© (ì§§ì€ ë¬¸ì¥ì€ 0ìœ¼ë¡œ ì±„ì›€)
        padded_training_indices = []
        for indices in training_indices:
            padded_indices = indices + [0] * (max_length - len(indices))  # ê³ ì •ëœ ê¸¸ì´ë¡œ ë§ì¶”ê¸°
            padded_training_indices.append(padded_indices)

        # âœ… 3. NumPy ë°°ì—´ë¡œ ë³€í™˜ í›„ Cupy ë°°ì—´ë¡œ ë³€í™˜
        padded_training_indices = cp.array(np.array(padded_training_indices, dtype=np.int32))

        for epoch in range(epochs):
            batch_losses = []

            # âœ… 4. ë°°ì¹˜ ìƒ˜í”Œë§ (ì˜¬ë°”ë¥¸ ë°©ì‹ìœ¼ë¡œ ë³€ê²½)
            batch_indices = cp.random.choice(len(padded_training_indices), size=batch_size, replace=False)
            batch_sentences = padded_training_indices[batch_indices]

            for indices in batch_sentences:
                for i in range(len(indices) - 1):
                    current_idx = int(indices[i])  # ğŸš€ ì •ìˆ˜ ë³€í™˜ (Cupy ì˜¤ë¥˜ ë°©ì§€)
                    next_idx = int(indices[i + 1])

                    # í˜„ì¬ ë¬¸ì ë‰´ëŸ° í™œì„±í™”
                    self.token_layer.neurons.potentials[current_idx] = 50.0

                    # ì—°ì‚°ì¸µ ì‹ í˜¸ ì „ë‹¬
                    for synapse in self.synapses:
                        synapse.propagate()

                    # ì¶œë ¥ì¸µ í™œì„±í™” ë° ë°œí™” ì—¬ë¶€ í™•ì¸
                    fired = self.output_layer.fire()

                    # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (ë°œí™”í•œ ë‰´ëŸ°ë§Œ í•™ìŠµ)
                    if fired[next_idx]:
                        for synapse in self.synapses:
                            synapse.weights[current_idx, next_idx] += 0.005  # ë” ì‘ì€ í•™ìŠµë¥  ì ìš©
                        batch_losses.append(1)  # ì„±ê³µí•œ ì¼€ì´ìŠ¤

                    # ë‰´ëŸ° ìƒíƒœ ì—…ë°ì´íŠ¸ (ì´ì˜¨ ë†ë„, ì´ì˜¨ íŒí”„ í¬í•¨)
                    self.token_layer.neurons.update()
                    for layer in self.processing_layers:
                        layer.update()
                    self.output_layer.update()

            # âœ… 5. í•™ìŠµ ì •í™•ë„ ì¶œë ¥
            if epoch % 100 == 0:
                avg_loss = sum(batch_losses) / max(len(batch_losses), 1)  # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒ ë°©ì§€
                print(f"Epoch {epoch+1}/{epochs} - í‰ê·  í•™ìŠµ ì •í™•ë„: {avg_loss:.4f}")
    
    def predict(self, text, length=10):
        """ë‹¤ìŒ ë¬¸ì ì˜ˆì¸¡"""
        tokens = self.token_layer.tokenize(text)
        indices = self.token_layer.encode(tokens)
        output_text = tokens

        for _ in range(length):
            current_idx = indices[-1]
            self.token_layer.neurons.potentials[current_idx] = 50.0

            # ì—°ì‚°ì¸µ ì‹ í˜¸ ì „ë‹¬
            for synapse in self.synapses:
                synapse.propagate()

            # ì¶œë ¥ì¸µì—ì„œ ê°€ì¥ ê°•í•œ ì‹ í˜¸ ì°¾ê¸°
            next_idx = int(cp.argmax(self.output_layer.potentials))  # ğŸš€ ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜

            output_text.append(self.token_layer.alphabet[next_idx])
            indices.append(next_idx)

            # ë‰´ëŸ° ì´ˆê¸°í™”
            self.token_layer.neurons.update()
            for layer in self.processing_layers:
                layer.update()
            self.output_layer.update()

        return self.token_layer.decode(indices)
    
    def compute_perplexity(self, text):
        """Perplexity ê³„ì‚° (GPT-2 ì„±ëŠ¥ í‰ê°€ ë°©ì‹)"""
        tokens = self.token_layer.tokenize(text)
        indices = self.token_layer.encode(tokens)
        loss = 0.0

        for i in range(len(indices) - 1):
            current_idx = indices[i]
            next_idx = indices[i + 1]

            prob = cp.exp(self.synapses[-1].weights[current_idx, next_idx])  # ì‹œëƒ…ìŠ¤ ê°€ì¤‘ì¹˜ë¥¼ í™•ë¥ ë¡œ ë³€í™˜
            loss += -cp.log(prob + 1e-9)  # ë¡œê·¸ ì†ì‹¤ ê³„ì‚° (ì•ˆì •ì„± ìœ„í•´ 1e-9 ì¶”ê°€)

        perplexity = cp.exp(loss / len(indices))  # í‰ê·  perplexity ê³„ì‚°
        return float(perplexity)

def load_large_text_data():
    """ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì˜ˆ: í”„ë¡œì íŠ¸ êµ¬í…ë² ë¥´í¬)"""
    urls = [
        "https://www.gutenberg.org/files/1342/1342-0.txt",  # Pride and Prejudice
        "https://www.gutenberg.org/files/2701/2701-0.txt",  # Moby Dick
        "https://www.gutenberg.org/files/11/11-0.txt"  # Alice in Wonderland
    ]
    texts = []

    for url in urls:
        response = requests.get(url)
        text_data = response.text
        sentences = text_data.split('.')
        texts.extend([s.strip().upper() for s in sentences if len(s.strip()) > 10])

    return texts

def main():
    """GPT-2 ìˆ˜ì¤€ì˜ ë°˜ë³µ í•™ìŠµ ì‹¤í–‰"""
    model = NLPModel(num_layers=6)  # ë” ê¹Šì€ ëª¨ë¸ ì‚¬ìš©

    # ëŒ€ê·œëª¨ ë°ì´í„° ë¡œë“œ
    training_texts = load_large_text_data()
    print(f"í›ˆë ¨ ì‹œì‘: {len(training_texts)}ê°œì˜ ë¬¸ì¥ í•™ìŠµ")

    model.train(training_texts, epochs=20000, batch_size=128)  # ì¥ê¸° í•™ìŠµ

    # ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
    test_text = "MACHINE LEARNING IS"
    predicted_text = model.predict(test_text, length=10)
    ppl = model.compute_perplexity(test_text)

    print(f"ì…ë ¥: {test_text}")
    print(f"ì˜ˆì¸¡ëœ í…ìŠ¤íŠ¸: {predicted_text}")
    print(f"Perplexity: {ppl:.4f}")

if __name__ == "__main__":
    main()
